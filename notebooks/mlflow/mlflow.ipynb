{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excess-reality",
   "metadata": {},
   "source": [
    "Inspired by: https://docs.databricks.com/_static/notebooks/mlflow/mlflow-quick-start-training.html\n",
    "# Useful links:\n",
    "- MLflow documentation: https://www.mlflow.org/docs/latest/index.html\n",
    "- MLflow guide from databricks: https://docs.databricks.com/applications/mlflow/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-harmony",
   "metadata": {},
   "source": [
    "# Notes (MLflow Tracking)\n",
    "documentation > https://www.mlflow.org/docs/latest/tracking.html\n",
    "\n",
    "- Can be used to to track experiments to record and compare parameters and results.\n",
    "\n",
    "**Vocabulary**\n",
    "- *run*: corresponds to a single execution of model training code. Each run can record different informations (model parameters, metrics, tags, artifacts, etc).\n",
    "- *experiment*: the primary unit of organization and access control for MLflow runs; all MLflow runs belong to an experiment. Experiments let you visualize, search for, and compare runs, as well as download run artifacts and metadata for analysis in other tools.\n",
    "- *MLflow entities*: runs, parameters, metrics, tags, notes, metadata, etc\n",
    "- ...\n",
    "\n",
    "**What can be recorded by an MLflow run?** > https://www.mlflow.org/docs/latest/tracking.html#concepts\n",
    "\n",
    "**Where runs are recorded?** > https://www.mlflow.org/docs/latest/tracking.html#where-runs-are-recorded\n",
    "\n",
    "They can be recorded\n",
    "- to local files (by default to *mlruns* directory)\n",
    "    - Launch UI: `mlflow ui`\n",
    "- to SQLAlchemy compatible database\n",
    "    - Setup MLflow: `mlflow.set_tracking_uri('sqlite:///mlflow.db')`\n",
    "    - Launch UI: `mlflow ui --backend-store-uri sqlite:///mlflow.db`\n",
    "- remotely to a tracking server\n",
    "\n",
    "To show the current tracking uri `mlflow.get_tracking_uri()`\n",
    "    \n",
    "**How they are recorded** > https://www.mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded\n",
    "\n",
    "MLflow uses two components for storage:\n",
    "- backend store: for MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc)\n",
    "- artifact store: for artifacts (files, models, images, in-memory objects, or model summary, etc)\n",
    "\n",
    "You can use either manual or automatic logging\n",
    "- Manual logging > https://www.mlflow.org/docs/latest/tracking.html#logging-functions\n",
    "    - Log the fitted model: `mlflow.sklearn.log_model(rf, 'random-forest-model')`\n",
    "    - Log the model parameters:\n",
    "        - One parameter at a time: `mlflow.log_param('num_trees', n_estimators)`\n",
    "        - A dict of parameters: `mlflow.log_parms({'num_trees', n_estimators, 'alpha', 0.04})`\n",
    "    - Log the evaluation metrics: `mlflow.log_metric('mse', mse)`\n",
    "    - Log other artifacts: `mlflow.log_artifact('predictions.csv')`\n",
    "\n",
    "- Automatic logging with MLflow autolog\n",
    "    - With MLflow's autologging capabilities, a single line of code automatically logs the resulting model, the parameters used to create the model, and a model score > https://www.mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "    - Call `mlflow.<framework>.autolog()` API before running training code to log model-specific metrics, parameters, and model artifacts. Supports many ML frameworks (sklearn, tensorflow, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-permit",
   "metadata": {},
   "source": [
    "# Hello world MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mlflow\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the name of the experiment\n",
    "mlflow.set_experiment('my_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset() -> pd.DataFrame:\n",
    "    db = load_diabetes()\n",
    "    X, y = db.data, db.target\n",
    "    return train_test_split(X, y, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_dataset()\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    #mlflow.log_metric('rmse', rmse)\n",
    "    #mlflow.log_metric('mae', mae)\n",
    "    #mlflow.log_metric('r2', r2)\n",
    "    mlflow.log_metrics({'rmse': rmse, 'mae': mae, 'r2': r2})\n",
    "    print(f'RMSE = {rmse:.2f}, MAE = {mae:.2f}, R2 = {r2:.2f}')\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def train_model(X_train, X_test, y_train, y_test: pd.DataFrame, model_class, **model_kwargs) -> int:\n",
    "    model = model_class(**model_kwargs)\n",
    "    mlflow.log_params(model_kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(model, 'elastic_net')\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-mauritius",
   "metadata": {},
   "source": [
    "- Setup the mlflow and use a tracking server\n",
    "    - Launch the server in your terminal: `mlflow server --backend-store-uri sqlite:////tmp/mlruns.db --default-artifact-root /tmp/mlruns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-extent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-cross",
   "metadata": {},
   "source": [
    "- When launching a *run*, always use a context manager (with statement) so that the *run* will be closed. Otherwise, you need to close it manually with *mlflow.end_run()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-standard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_kwargs = {'alpha': 0.01, 'l1_ratio': 0.75}\n",
    "    train_model(X_train, X_test, y_train, y_test, ElasticNet, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_kwargs = {'alpha': 0.02, 'l1_ratio': 0.7}\n",
    "    train_model(X_train, X_test, y_train, y_test, ElasticNet, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_kwargs = {'alpha': 0.1, 'l1_ratio': 0.01}\n",
    "    train_model(X_train, X_test, y_train, y_test, ElasticNet, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_kwargs = {'alpha': 0.005, 'l1_ratio': 0.8}\n",
    "    train_model(X_train, X_test, y_train, y_test, ElasticNet, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-degree",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow.search_runs(filter_string=\"metric.rmse < 60\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
